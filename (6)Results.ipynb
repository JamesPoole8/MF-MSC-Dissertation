{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7c47cc-44ab-4ad6-ae39-76dc4d802d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a65ddd-1e58-4f8a-91b5-964d2318d658",
   "metadata": {},
   "source": [
    "## Load Data from Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de30297d-3fd5-499d-8898-71ca6f2d7b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data from the SAI method\n",
    "ret_bench = pd.read_csv('ret_bench.csv', index_col='Date')\n",
    "ret_raw = pd.read_csv('raw_returns.csv', index_col='Date')  \n",
    "ret_extended = pd.read_csv('extended_returns.csv', index_col='Date')  \n",
    "ret_featuretools = pd.read_csv('featuretools_extended_returns.csv', index_col='Date')  \n",
    "ret_tsfresh = pd.read_csv('tsfresh_extended_returns.csv', index_col='Date')  \n",
    "ret_featurewiz = pd.read_csv('featurewiz_extended_returns.csv', index_col='Date') \n",
    "ret_pycaret = pd.read_csv('pycaret_extended_returns.csv', index_col='Date')  \n",
    "ret_optimal = pd.read_csv('ret_opt.csv', index_col='Date')  \n",
    "\n",
    "siz_bench = pd.read_csv('siz_bench.csv', index_col='Date')\n",
    "siz_raw = pd.read_csv('raw_sizes.csv', index_col='Date')  \n",
    "siz_extended = pd.read_csv('extended_sizes.csv', index_col='Date') \n",
    "siz_featuretools = pd.read_csv('featuretools_extended_sizes.csv', index_col='Date') \n",
    "siz_tsfresh = pd.read_csv('tsfresh_extended_sizes.csv', index_col='Date')  \n",
    "siz_featurewiz = pd.read_csv('featurewiz_extended_sizes.csv', index_col='Date')  \n",
    "siz_pycaret = pd.read_csv('pycaret_extended_sizes.csv', index_col='Date')  \n",
    "siz_optimal = pd.read_csv('siz_opt.csv', index_col='Date')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d95fe1-de96-4615-984e-1ce51794e70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionaries to hold all the return and size DataFrames\n",
    "return_datasets = {\n",
    "    'Benchmark': ret_bench,\n",
    "    'Raw': ret_raw,\n",
    "    'Extended': ret_extended,\n",
    "    'Featuretools': ret_featuretools,\n",
    "    'TSFresh': ret_tsfresh,\n",
    "    'Featurewiz': ret_featurewiz,\n",
    "    'PyCaret': ret_pycaret,\n",
    "    'Optimal': ret_optimal\n",
    "}\n",
    "\n",
    "size_datasets = {\n",
    "    'Benchmark': siz_bench,\n",
    "    'Raw': siz_raw,\n",
    "    'Extended': siz_extended,\n",
    "    'Featuretools': siz_featuretools,\n",
    "    'TSFresh': siz_tsfresh,\n",
    "    'Featurewiz': siz_featurewiz,\n",
    "    'PyCaret': siz_pycaret,\n",
    "    'Optimal': siz_optimal\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da914b98-baf5-4519-a31b-04c36888fae3",
   "metadata": {},
   "source": [
    "## Calculate Portfolio Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7867ecaa-737f-466e-b966-0104cadefd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate portfolio metrics for a given set of returns and sizes\n",
    "def calculate_metrics(returns, sizes, benchmark_returns):\n",
    "    # Define metrics\n",
    "    metrics = [\n",
    "        'Annualised Return', \n",
    "        'STD of Return', \n",
    "        'Mean # of Equities', \n",
    "        'Sharpe Ratio',\n",
    "        'Sharpe Ratio p-value', \n",
    "        'Tracking Error', \n",
    "        'Info Ratio',\n",
    "        'Info Ratio p-value'\n",
    "    ]\n",
    "    \n",
    "    # Create a DataFrame to store the results\n",
    "    results = pd.DataFrame(index=metrics, columns=returns.columns)\n",
    "\n",
    "    # Calculate annualised returns \n",
    "    total_ret = (returns.prod(axis=0) ** (1 /((len(returns)/4)-0.25))) - 1\n",
    "    results.loc['Annualised Return'] = total_ret\n",
    "\n",
    "    # Calculate annualised standard deviation of returns\n",
    "    ret_std = returns.std(axis=0)*2  \n",
    "    results.loc['STD of Return'] = ret_std\n",
    "\n",
    "    # Calculate mean portfolio size\n",
    "    mean_size = sizes.mean(axis=0)\n",
    "    results.loc['Mean # of Equities'] = mean_size\n",
    "\n",
    "    # Calculate Sharpe ratios (with excess returns)\n",
    "    sharpe_ratios = total_ret / ret_std\n",
    "    results.loc['Sharpe Ratio'] = sharpe_ratios\n",
    "\n",
    "    # Calculate tracking error using the benchmark\n",
    "    relative_ret = returns.sub(benchmark_returns.values, axis=0)\n",
    "    tracking_error = relative_ret.std(axis=0) * np.sqrt(4)  \n",
    "    results.loc['Tracking Error'] = tracking_error\n",
    "\n",
    "    # Calculate information ratio\n",
    "    annualized_rel_ret = relative_ret.mean(axis=0) * 4 \n",
    "    info_ratio = annualized_rel_ret / tracking_error\n",
    "    results.loc['Info Ratio'] = info_ratio\n",
    "\n",
    "    # Perform t-tests for Sharpe Ratio and Information Ratio\n",
    "    sharpe_t_stat = sharpe_ratios * np.sqrt(len(returns)) / (ret_std)\n",
    "    sharpe_p_values = stats.t.sf(sharpe_t_stat, df=len(returns)-1)  \n",
    "\n",
    "    info_t_stat = info_ratio * np.sqrt(len(returns)) / (tracking_error)\n",
    "    info_p_values = stats.t.sf(info_t_stat, df=len(returns)-1) \n",
    "\n",
    "    # Add p-values to the results\n",
    "    results.loc['Sharpe Ratio p-value'] = sharpe_p_values\n",
    "    results.loc['Info Ratio p-value'] = info_p_values\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89757c5-80aa-4d9c-9c17-8046dd253083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the simulation on each dataset and compile performance metrics\n",
    "def compile_metrics(datasets, sizes, benchmark_returns):\n",
    "\n",
    "    compiled_results = {}\n",
    "\n",
    "    for name in datasets.keys():\n",
    "        # Extract chi column \n",
    "        chi_returns = datasets[name]['chi'].to_frame(name=name)\n",
    "        chi_sizes = sizes[name]['chi'].to_frame(name=name)\n",
    "\n",
    "        # Calculate metrics \n",
    "        results = calculate_metrics(chi_returns, chi_sizes, benchmark_returns)\n",
    "\n",
    "        # Store results\n",
    "        compiled_results[name] = results\n",
    "\n",
    "    # Concatenate all results into a single dataset\n",
    "    final_results = pd.concat(compiled_results, axis=1)\n",
    "\n",
    "    return final_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a153ea-d52a-462f-92ce-bec901f82b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the benchmark returns\n",
    "benchmark_returns = ret_bench['chi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0810f107-6277-4ab4-ba5b-132524754e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run analysis on all datasets\n",
    "final_results = compile_metrics(return_datasets, size_datasets, benchmark_returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a745dff9-4b1f-4639-86ff-460fd100a080",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_results_table(results):\n",
    "    # Format Dataframe to 4 sig fig\n",
    "    results = results.map(lambda x: f'{x:.4g}' if pd.notnull(x) else x)\n",
    "\n",
    "    # Convert returns and standard deviations to percentages\n",
    "    results.loc['Annualised Return'] = results.loc['Annualised Return'].astype(float) * 100\n",
    "    results.loc['Annualised Return'] = results.loc['Annualised Return'].apply(lambda x: f'{x:.2f}%')\n",
    "    \n",
    "    results.loc['STD of Return'] = results.loc['STD of Return'].astype(float) * 100\n",
    "    results.loc['STD of Return'] = results.loc['STD of Return'].apply(lambda x: f'{x:.2f}%')\n",
    "        \n",
    "    # Format benchmark column\n",
    "    results.loc[['Tracking Error', 'Info Ratio', 'Info Ratio p-value'], 'Benchmark'] = 'NA'\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692bc098-c99e-4b88-a60e-2b563f63959b",
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_results = format_results_table(final_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc81f353-3318-4967-817f-a3b78e2ef175",
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_results.to_csv('Final_results.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
