{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04fb3d05-b456-4f3e-ad1b-fc198ebf771b",
   "metadata": {},
   "source": [
    "This notebook was built utilising code from the work of Birks in his dissertation, refer to it for more information:\n",
    "\n",
    "Birks, J. (2023). “Interpretable AI in portfolio management: Ensemble rule models and causal\n",
    "pruning via large language models”. Master’s dissertation. University of Warwick."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b85ba73-1f88-469b-ad90-2f079a26638c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import Counter\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import fpgrowth, association_rules\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57d7699-3f87-4b5d-89e8-e3a7c53508b4",
   "metadata": {},
   "source": [
    "## ARM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9635ee2-966a-47d0-9ca9-a2f02cd88d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that ranks data into quintiles\n",
    "def rank_factors(info):\n",
    "    for item in info.columns.tolist():\n",
    "        string = item + \" Rank\"\n",
    "        info[string] = pd.qcut(info[item], 5, labels=[item + \"1\", item + \"2\", item + \"3\", item + \"4\", item + \"5\"])\n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b409f1d2-9f54-41dc-948b-aacedaf393dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that creates an array of lists of factors for each available asset\n",
    "def fpg_prep(info):\n",
    "    # Drop the columns that do not contain the rankings\n",
    "    state = info.drop([col for col in info.columns if 'Rank' not in col and pd.api.types.is_numeric_dtype(info[col])], axis=1)\n",
    "    # Drop the returns ranked column and assign remaining info to new \n",
    "    new = state.drop(\"Return Rank\", axis=1)\n",
    "    # Reset the index of state and drop the names column\n",
    "    state = state.reset_index()\n",
    "    final = []\n",
    "    # For each row, append final with each row as an array of its own\n",
    "    for i in range(0, len(state)):\n",
    "        final.append(state.loc[i, state.columns[1:]].tolist())\n",
    "    # Return both final and new\n",
    "    return final, new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e924663-6483-4462-8506-cae94c0852f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that mines association and lift rules, the 5th quintile returns and the true/false df for the FPG algorithm\n",
    "def rules(final, sup, conf):\n",
    "    # Preprocessing of input argument into true and false for each discretisation\n",
    "    te = TransactionEncoder()\n",
    "    te_ary = te.fit(final).transform(final)\n",
    "    # Eg is the true/false dataframe in this form for the fp growth algorithm\n",
    "    eg = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "    # True/false column of the highest quintile of returns\n",
    "    high_returns = eg.Return5\n",
    "    # Finding frequent items in data for a minimum support of 5%\n",
    "    freq_items = fpgrowth(eg, min_support=sup, use_colnames=True)\n",
    "    # Discover association and causal rules\n",
    "    asso_rules = association_rules(freq_items, metric=\"confidence\", min_threshold=conf)\n",
    "    lift_rules = association_rules(freq_items, metric=\"lift\", min_threshold=1.2)\n",
    "    # Return true/false dataframe, high returns, and the found\n",
    "    return eg, high_returns, asso_rules, lift_rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ed0c9a-b4c8-4b21-8776-8ee2dca1236a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get high return rules from rule set\n",
    "def high_ret_rules(asso_rules):\n",
    "    factors = []\n",
    "    for index in asso_rules.index.tolist():\n",
    "        if set([\"Return5\"]).issubset(set(list(asso_rules.loc[index, 'consequents']))):\n",
    "            factors.append(list(asso_rules.loc[index, 'antecedents']))\n",
    "    return factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1e9247-5b1e-4d01-831c-b568c0c17c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get rules\n",
    "def get_rules(unique_asso):\n",
    "    associatons = []\n",
    "    for asso in unique_asso:\n",
    "        if not isinstance(asso, list):\n",
    "            associatons.append([asso])\n",
    "        else:\n",
    "            associatons.append(asso)\n",
    "    return associatons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4923a195-7f67-44eb-b7a7-26db21955ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get rules for the period, by using the above functions\n",
    "def rules_for_period(info, sup, conf):\n",
    "    info = rank_factors(info.iloc[:, 2:])\n",
    "    final, new = fpg_prep(info)\n",
    "    eg, high_returns, asso_rules, lift_rules = rules(final, sup, conf)\n",
    "    factors = high_ret_rules(asso_rules)\n",
    "    \n",
    "    # Drop only the ranking columns but keep the original data columns\n",
    "    info = info.drop([col for col in info.columns if 'Rank' not in col and pd.api.types.is_numeric_dtype(info[col])], axis=1)\n",
    "    \n",
    "    # Get rule set\n",
    "    rules_set = get_rules(factors)\n",
    "    \n",
    "    # Remove duplicate rules and return them as a list\n",
    "    unique_rules = list(np.unique(rules_set))\n",
    "    \n",
    "    return unique_rules, eg, high_returns, info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c38a586-6d2d-45d7-9810-8cf2c3ac2566",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Chi Squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfce1512-a1af-4905-a6ad-db5f22ff0db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6751bdd3-3085-4c69-86f7-23ef90815758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get expected and actual frequency of a rule\n",
    "def expected_freq(info, rule):\n",
    "    # Get number of equities with top quintile returns\n",
    "    num_ret_ind = len(info[info['Return Rank'] == 'Return5'])\n",
    "    \n",
    "    # Calculate expected frequency\n",
    "    mask = info.isin(rule)\n",
    "    filtered = info[mask].dropna(axis=0, how='all')\n",
    "    num_rule_ind = len(filtered)\n",
    "    data_len = len(info)\n",
    "    ef_ind = (num_ret_ind / data_len) * (num_rule_ind / data_len) * data_len\n",
    "    \n",
    "    # Calculate actual frequency\n",
    "    rule.append('Return5')\n",
    "    mask = info.isin(rule)\n",
    "    filtered = info[mask]\n",
    "    filtered = filtered.dropna(thresh=len(rule))\n",
    "    actual_freq = len(filtered)\n",
    "    rule.pop(-1)\n",
    "    return actual_freq, ef_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5e4660-ad1f-4e0c-aaed-e4728e0fd072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make list of strings into list of lists, where each string becomes its own list\n",
    "def list_list(lis):\n",
    "    \n",
    "    list_of_lists = []\n",
    "    \n",
    "    for string in lis:\n",
    "        # Create a new list containing the current string\n",
    "        new_list = [string]\n",
    "        # Add the new list to the list_of_lists\n",
    "        list_of_lists.append(new_list)\n",
    "    \n",
    "    return list_of_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdf0ef6-d6dd-4145-ad1b-3e09b76d5212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique elements for a list\n",
    "def get_unique_elements(input_list):\n",
    "    unique_elements = []\n",
    "    for element in input_list:\n",
    "        if element not in unique_elements:\n",
    "            unique_elements.append(element)\n",
    "    return unique_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8fc2be-dee1-45e8-9c72-d6927e56b0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get rules that pass the chi-squared pruning\n",
    "def causal_chi(info, rules):\n",
    "    if len(rules) == 0:\n",
    "        return []\n",
    "    causal = []\n",
    "    chi_stats = []\n",
    "    # Append rules that have a significant Chi-stat\n",
    "    for rule in rules:\n",
    "        if type(rule) == np.str_:\n",
    "            rule = [rule]\n",
    "        actual_freq, ef_ind = expected_freq(info, rule)\n",
    "        stat = ((actual_freq - ef_ind) ** 2) / ef_ind\n",
    "        dof = len(info['Return Rank'].unique()) - 1\n",
    "        p = stats.chi2.cdf(stat, dof)\n",
    "        if p > 0.95:\n",
    "            causal.append(rule)\n",
    "            chi_stats.append(stat)\n",
    "    \n",
    "    if rules == causal or list_list(rules) == causal:\n",
    "        causal2 = []\n",
    "        index_1 = chi_stats.index(max(chi_stats))\n",
    "        causal2.append(causal[index_1])\n",
    "        chi_stats[index_1] = 0\n",
    "        \n",
    "        index_2 = chi_stats.index(max(chi_stats))\n",
    "        causal2.append(causal[index_2])\n",
    "        chi_stats[index_2] = 0\n",
    "        \n",
    "        index_3 = chi_stats.index(max(chi_stats))\n",
    "        causal2.append(causal[index_3])\n",
    "        chi_stats[index_3] = 0\n",
    "        \n",
    "        return get_unique_elements(causal2)\n",
    "        \n",
    "    return causal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc36c02-696f-4dec-bb75-96cf198eee33",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7095866b-d737-4013-b7d6-9e5e08a0f31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find the n most frequent items from a list\n",
    "def find_n_most_frequent_items(lst, N):\n",
    "    random.shuffle(lst)\n",
    "    item_counts = Counter(lst)\n",
    "    most_common_items = item_counts.most_common()\n",
    "    N_ind = min((len(most_common_items) - 1), (N - 1))\n",
    "    max_count = most_common_items[N_ind][1]\n",
    "    result1 = [item[0] for item in most_common_items if item[1] > max_count]\n",
    "    result2 = [item[0] for item in most_common_items if item[1] == max_count]\n",
    "    return result1, result2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3427110-7bb3-4e17-bf75-df01b7e88da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function, given a ruleset finds the picks the equities due to those rules\n",
    "def get_equities_data(rules, ranked_data, cur_data):\n",
    "    if len(rules) == 0:\n",
    "        results = [(cur_data['Return'].mean() + 1), len(cur_data)]\n",
    "        return results\n",
    "    else:\n",
    "        ranked_data.reset_index()\n",
    "        stocks = []\n",
    "        number_of_rules = len(rules)\n",
    "        max_stocks = round(len(ranked_data) * 0.25)\n",
    "        for rule in rules:\n",
    "            if type(rule) == np.str_:\n",
    "                rule = [rule]\n",
    "            mask = ranked_data.isin(rule)\n",
    "            filtered = ranked_data[mask]\n",
    "            filtered = filtered.dropna(thresh=len(rule))\n",
    "            stocks += filtered.index.tolist()\n",
    "        stocks1, stocks2 = find_n_most_frequent_items(stocks, round(max_stocks))\n",
    "        cur_data2 = cur_data[cur_data.index.isin(stocks1)]\n",
    "        cur_data3 = cur_data[cur_data.index.isin(stocks2)]\n",
    "        if len(stocks1) > 0 and len(stocks2) > 0:\n",
    "            mean_returns = (cur_data2['Return'].mean() + 1) * (len(stocks1) / max_stocks) + (cur_data3['Return'].mean() + 1) * (1 - (len(stocks1) / max_stocks))\n",
    "        elif len(stocks1) > 0 and len(stocks2) == 0:\n",
    "            mean_returns = (cur_data2['Return'].mean() + 1)\n",
    "        elif len(stocks1) == 0 and len(stocks2) > 0:\n",
    "            mean_returns = (cur_data3['Return'].mean() + 1)\n",
    "        results = [mean_returns, max_stocks]\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01f9781-a250-49cf-ad5c-58e0f65cf508",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulation(data, periods, sup, conf):\n",
    "    dates = data.index.unique()\n",
    "    column_names = ['chi']  \n",
    "    returns_df = pd.DataFrame(index=dates[5:-periods], columns=column_names)\n",
    "    size_df = pd.DataFrame(index=dates[5:-periods], columns=column_names)\n",
    "    \n",
    "    half_period_plus_1 = (periods / 2) + 1\n",
    "    \n",
    "    for i in range(5, len(dates) - periods):\n",
    "        # Get rolling window data \n",
    "        window_data = data.loc[dates[i:i + periods + 1]]\n",
    "        \n",
    "        # Filter window data where more than periods/2 periods are available for each stock\n",
    "        stock_counts = window_data['Name'].value_counts()\n",
    "        valid_stocks = stock_counts[stock_counts > half_period_plus_1].index\n",
    "        window_data = window_data[window_data['Name'].isin(valid_stocks)]\n",
    "        \n",
    "        # Get current period data\n",
    "        current_data = window_data.loc[dates[i + periods]].set_index('ID')\n",
    "        \n",
    "        # Drop current period data from window data\n",
    "        window_data = window_data[window_data.index.isin(dates[i:i + periods])]\n",
    "        \n",
    "        # Get the ARM and CRM rulesets\n",
    "        assoc_rules, eg, ret, info = rules_for_period(window_data, sup, conf)\n",
    "        \n",
    "        # Apply Chi-Squared pruning\n",
    "        chi = causal_chi(info, assoc_rules)\n",
    "        \n",
    "        # Encode current data and rank factors\n",
    "        info2 = rank_factors(current_data.iloc[:, 1:])\n",
    "        info2 = info2.filter(like='Rank', axis=1)\n",
    "        \n",
    "        # Get the returns of each rule set \n",
    "        chi_equities = get_equities_data(chi, info2, current_data)\n",
    "        \n",
    "        # Store results in dataframes\n",
    "        returns_df.iloc[i - 5, returns_df.columns.get_loc('chi')] = chi_equities[0]\n",
    "        size_df.iloc[i - 5, size_df.columns.get_loc('chi')] = chi_equities[1]\n",
    "    \n",
    "    return returns_df, size_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a72b43-9ce3-4f79-a9bd-5096a3a7ce5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark(data, periods):\n",
    "    dates = data.index.unique()\n",
    "    column_names = ['chi']\n",
    "    returns_df = pd.DataFrame(index=dates[5:-periods], columns=column_names)\n",
    "    size_df = pd.DataFrame(index=dates[5:-periods], columns=column_names)\n",
    "\n",
    "    half_period_plus_1 = (periods / 2) + 1\n",
    "\n",
    "    for i in range(5, len(dates) - periods):\n",
    "        # Get rolling window data\n",
    "        window_data = data.loc[dates[i:i + periods + 1]]\n",
    "        \n",
    "        # Filter window data where more than periods/2 periods are available for each stock\n",
    "        stock_counts = window_data['Name'].value_counts()\n",
    "        valid_stocks = stock_counts[stock_counts > half_period_plus_1].index\n",
    "        window_data = window_data[window_data['Name'].isin(valid_stocks)]\n",
    "        \n",
    "        # Get current period data\n",
    "        current_data = window_data.loc[dates[i + periods]].set_index('ID')\n",
    "        \n",
    "        # Drop current period data from window data\n",
    "        window_data = window_data[window_data.index.isin(dates[i:i + periods])]\n",
    "        \n",
    "        # Calculate benchmark return\n",
    "        benchmark_return = current_data['Return'].mean() + 1\n",
    "        \n",
    "        # Store results in dataframes\n",
    "        returns_df.iloc[i - 5, returns_df.columns.get_loc('chi')] = benchmark_return\n",
    "        size_df.iloc[i - 5, size_df.columns.get_loc('chi')] = len(current_data)\n",
    "    \n",
    "    return returns_df, size_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc43a37f-4641-442a-9805-5b171b1d9629",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Run the Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efc1e65-eea6-42e0-a506-ed27c212ac9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b137c542-9791-496c-8915-1e1181fb80ff",
   "metadata": {},
   "source": [
    "### Clean the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f5ada1-a7f3-47b0-911a-d93736eda097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary of loaded datasets\n",
    "datasets = {\n",
    "    'raw': pd.read_csv('cleaned_raw.csv', index_col='Date'),\n",
    "    'extended': pd.read_csv('cleaned_extended.csv', index_col='Date'),\n",
    "    'featuretools_extended': pd.read_csv('cleaned_featuretools_extended.csv', index_col='Date'),\n",
    "    'tsfresh_extended': pd.read_csv('cleaned_tsfresh_extended.csv', index_col='Date'),\n",
    "    'featurewiz_extended': pd.read_csv('cleaned_featurewiz_extended.csv', index_col='Date'),\n",
    "    'pycaret_extended': pd.read_csv('cleaned_pycaret_extended.csv', index_col='Date')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7e87fc-4c16-4235-ac04-88bfe3b22362",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = datasets['raw']\n",
    "extended = datasets['extended']\n",
    "\n",
    "raw = raw.drop(columns=['Unnamed: 0'])\n",
    "extended = extended.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "datasets['raw'] = raw \n",
    "datasets['extended'] = extended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc6727f-b9e1-422f-842c-7ac20d601b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean nearly constant columns\n",
    "def drop_cum_columns(df):\n",
    "    # Drop columns that contain 'CUM' in their names\n",
    "    columns_to_drop = [col for col in df.columns if 'CUM' in col]\n",
    "    df = df.drop(columns=columns_to_drop)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Clean nearly constant columns\n",
    "def drop_date_columns(df):\n",
    "    # Drop columns that contain 'Date' in their names\n",
    "    columns_to_drop = [col for col in df.columns if 'Date' in col]\n",
    "    df = df.drop(columns=columns_to_drop)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30469b1-544a-483d-b059-8c34a06949db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean problematic columns\n",
    "ft_ext = datasets['featuretools_extended']\n",
    "\n",
    "ft_ext = drop_cum_columns(ft_ext)\n",
    "\n",
    "ft_ext = drop_date_columns(ft_ext)\n",
    "\n",
    "ft_ext2 = ft_ext.drop(columns=['MODE(metrics_data.ID)', 'MODE(metrics_data.Name)'])\n",
    "                     \n",
    "datasets['featuretools_extended'] = ft_ext2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491fe24d-7cb6-4763-8b60-a7dc35680504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean problematic columns\n",
    "tf_ext = datasets['tsfresh_extended']\n",
    "\n",
    "tf_ext2 = tf_ext.drop(columns=['value__has_duplicate_min', 'value__sum_of_reoccurring_values', \n",
    "                              'value__sum_of_reoccurring_data_points', 'value__value_count__value_1', \n",
    "                              'value__value_count__value_-1','value__has_duplicate', 'value__longest_strike_below_mean', \n",
    "                              'value__count_above_mean', 'value__count_below_mean', 'value__last_location_of_maximum', 'value__first_location_of_maximum', \n",
    "                              'value__symmetry_looking__r_0.05', 'value__large_standard_deviation__r_0.2', 'value__number_cwt_peaks__n_1', 'value__number_peaks__n_5', \n",
    "                              'value__index_mass_quantile__q_0.2', 'value__index_mass_quantile__q_0.3', 'value__index_mass_quantile__q_0.4', 'value__index_mass_quantile__q_0.6', \n",
    "                              'value__index_mass_quantile__q_0.7', 'value__index_mass_quantile__q_0.8','value__index_mass_quantile__q_0.1', 'value__index_mass_quantile__q_0.9',\n",
    "                              'value__last_location_of_minimum', 'value__first_location_of_minimum', 'value__number_cwt_peaks__n_5', 'value__number_peaks__n_1', 'value__number_peaks__n_3', \n",
    "                               'value__change_quantiles__f_agg_\"var\"__isabs_False__qh_0.2__ql_0.0', 'value__change_quantiles__f_agg_\"var\"__isabs_True__qh_0.2__ql_0.0', \n",
    "                               'value__change_quantiles__f_agg_\"var\"__isabs_False__qh_0.4__ql_0.2', 'value__change_quantiles__f_agg_\"var\"__isabs_True__qh_0.4__ql_0.2', \n",
    "                               'value__change_quantiles__f_agg_\"var\"__isabs_False__qh_0.6__ql_0.4', 'value__change_quantiles__f_agg_\"mean\"__isabs_True__qh_0.6__ql_0.4', \n",
    "                               'value__change_quantiles__f_agg_\"var\"__isabs_True__qh_0.6__ql_0.4', 'value__change_quantiles__f_agg_\"var\"__isabs_False__qh_0.8__ql_0.6', \n",
    "                               'value__change_quantiles__f_agg_\"var\"__isabs_True__qh_0.8__ql_0.6', 'value__change_quantiles__f_agg_\"var\"__isabs_False__qh_1.0__ql_0.8', \n",
    "                               'value__change_quantiles__f_agg_\"var\"__isabs_True__qh_1.0__ql_0.8', 'value__value_count__value_0', 'value__number_crossing_m__m_0', \n",
    "                               'value__number_crossing_m__m_-1', 'value__fourier_entropy__bins_3', 'value__fourier_entropy__bins_5', 'value__fourier_entropy__bins_10', 'value__fourier_entropy__bins_100'])\n",
    "                     \n",
    "datasets['tsfresh_extended'] = tf_ext2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e066215f-ea09-4820-8f18-e35f914fabd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to drop low variance columns\n",
    "def drop_low_variance(df, threshold=0.02):\n",
    "    # Select only numeric columns\n",
    "    numeric_df = df.select_dtypes(include=[np.number])\n",
    "    \n",
    "    # Drop low variance columns\n",
    "    selector = VarianceThreshold(threshold=threshold)\n",
    "    selector.fit(numeric_df)\n",
    "    \n",
    "    # Get the mask of retained columns\n",
    "    retained_columns = numeric_df.columns[selector.get_support()]\n",
    "    \n",
    "    # Retain only the columns that passed the variance threshold\n",
    "    df_cleaned = df[retained_columns]\n",
    "    \n",
    "    # Add back any non-numeric columns\n",
    "    non_numeric_df = df.select_dtypes(exclude=[np.number])\n",
    "    df_cleaned = pd.concat([non_numeric_df, df_cleaned], axis=1)\n",
    "    \n",
    "    return df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7feb93a-9529-4949-af41-9fd536301a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to each dataset\n",
    "cleaned_datasets = {name: drop_low_variance(df) for name, df in datasets.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340648d8-8c4b-46a5-8c58-84b0beb9c89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, mutual_info_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020ccf71-9c9e-44da-a171-6ab7c08bc77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MI_reduction(df, target_column='Return', k=10):\n",
    "\n",
    "    # Separate the target variable\n",
    "    target = df[target_column]\n",
    "    \n",
    "    # Separate the 'Name' and 'ID' columns\n",
    "    non_numeric_columns = df[['Name', 'ID']]\n",
    "    \n",
    "    # Separate features and remove non-numeric columns\n",
    "    features = df.drop(columns=[target_column, 'Name', 'ID'])\n",
    "    features_numeric = features.select_dtypes(include=[float, int])\n",
    "    \n",
    "    # Standardize the features \n",
    "    scaler = StandardScaler()\n",
    "    scaled_features = scaler.fit_transform(features_numeric)\n",
    "    \n",
    "    # Apply SelectKBest \n",
    "    selector = SelectKBest(mutual_info_regression, k=k)\n",
    "    selected_features = selector.fit_transform(scaled_features, target)\n",
    "    \n",
    "    # Get the names of the selected features\n",
    "    selected_feature_names = features_numeric.columns[selector.get_support()]\n",
    "    \n",
    "    # Create a DataFrame with the selected features and add the target and non-numeric columns back\n",
    "    reduced_df = pd.DataFrame(selected_features, columns=selected_feature_names, index=df.index)\n",
    "    reduced_df[target_column] = target\n",
    "    reduced_df = pd.concat([non_numeric_columns, reduced_df], axis=1)\n",
    "    \n",
    "    return reduced_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc13eb4d-702c-49f7-a56f-02810510a8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store reduced datasets\n",
    "reduced_datasets = {}\n",
    "\n",
    "# Number of features to keep for each dataset\n",
    "features_to_keep = {\n",
    "    'raw': 10,\n",
    "    'extended': 20,\n",
    "    'featuretools_extended': 40,\n",
    "    'tsfresh_extended': 40,\n",
    "    'featurewiz_extended': 70,\n",
    "    'pycaret_extended': 80\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b36285-ecd8-4ad1-9f64-9f9d35da7934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the mutual information reduction to each dataset\n",
    "for name, df in cleaned_datasets.items():\n",
    "    k = features_to_keep[name]  \n",
    "    reduced_df = MI_reduction(df, k=k)\n",
    "    reduced_datasets[name] = reduced_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16fd079-06e1-48a6-ae0f-affb35f9edeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save each reduced dataset to a CSV file\n",
    "for name, df in reduced_datasets.items():\n",
    "    df.to_csv(f'reduced_{name}.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72830f0d-9dc0-47fe-9e95-59c2dfb05426",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Apply SAI Method to Each Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3257d5-c0b7-48f8-b320-83091cb7faef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary of loaded datasets\n",
    "reduced_datasets = {\n",
    "    'raw': pd.read_csv('reduced_raw.csv', index_col='Date'),\n",
    "    'extended': pd.read_csv('reduced_extended.csv', index_col='Date'),\n",
    "    'featuretools_extended': pd.read_csv('reduced_featuretools_extended.csv', index_col='Date'),\n",
    "    'tsfresh_extended': pd.read_csv('reduced_tsfresh_extended.csv', index_col='Date'),\n",
    "    'featurewiz_extended': pd.read_csv('reduced_featurewiz_extended.csv', index_col='Date'),\n",
    "    'pycaret_extended': pd.read_csv('reduced_pycaret_extended.csv', index_col='Date')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b46647-088b-47c4-815f-4fc001c149e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw dataset\n",
    "returns_raw, size_raw = simulation(reduced_datasets['raw'], periods=12, sup=0.05, conf=0.20)\n",
    "returns_raw.to_csv('raw_returns.csv')\n",
    "size_raw.to_csv('raw_sizes.csv')\n",
    "print(\"Raw dataset simulation completed and results saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdffe1d0-0274-4e0e-b2e1-d387fc2f30c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extended dataset\n",
    "returns_extended, size_extended = simulation(reduced_datasets['extended'], periods=12, sup=0.05, conf=0.20)\n",
    "returns_extended.to_csv('extended_returns.csv')\n",
    "size_extended.to_csv('extended_sizes.csv')\n",
    "print(\"Extended dataset simulation completed and results saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd75964-ed86-4240-afba-fbeb6dedb636",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Featuretools Extended dataset\n",
    "returns_featuretools_extended, size_featuretools_extended = simulation(reduced_datasets['featuretools_extended'], periods=12, sup=0.05, conf=0.20)\n",
    "returns_featuretools_extended.to_csv('featuretools_extended_returns.csv')\n",
    "size_featuretools_extended.to_csv('featuretools_extended_sizes.csv')\n",
    "print(\"Featuretools Extended dataset simulation completed and results saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc79fd6-3f9b-4657-97bf-86508a83a761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TSFRESH Extended dataset\n",
    "returns_tsfresh_extended, size_tsfresh_extended = simulation(reduced_datasets['tsfresh_extended'], periods=12, sup=0.05, conf=0.20)\n",
    "returns_tsfresh_extended.to_csv('tsfresh_extended_returns.csv')\n",
    "size_tsfresh_extended.to_csv('tsfresh_extended_sizes.csv')\n",
    "print(\"TSFRESH Extended dataset simulation completed and results saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51b539b-69d1-4c33-a7ce-0799efd85168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FeatureWiz Extended dataset\n",
    "returns_featurewiz_extended, size_featurewiz_extended = simulation(reduced_datasets['featurewiz_extended'], periods=12, sup=0.05, conf=0.20)\n",
    "returns_featurewiz_extended.to_csv('featurewiz_extended_returns.csv')\n",
    "size_featurewiz_extended.to_csv('featurewiz_extended_sizes.csv')\n",
    "print(\"FeatureWiz Extended dataset simulation completed and results saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32535951-88fa-4619-aab5-487d5d2281f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyCaret Extended dataset\n",
    "returns_pycaret_extended, size_pycaret_extended = simulation(reduced_datasets['pycaret_extended'], periods=12, sup=0.05, conf=0.20)\n",
    "returns_pycaret_extended.to_csv('pycaret_extended_returns.csv')\n",
    "size_pycaret_extended.to_csv('pycaret_extended_sizes.csv')\n",
    "print(\"PyCaret Extended dataset simulation completed and results saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d870e0-d138-4e86-a15e-8d71bbbd3d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the benchmark portfolio returns and size\n",
    "ret_bench, size_bench = benchmark(datasets['raw'],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a492402f-7458-40e4-a963-4c270519a12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save benchmark results to CSV\n",
    "ret_bench.to_csv('ret_bench.csv', index=True)\n",
    "size_bench.to_csv('siz_bench.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fb4c74-6cf3-4e9a-a60b-a05775138a8c",
   "metadata": {},
   "source": [
    "## Simulate Optimal Set (After running notebook 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2633f131-2063-4005-969d-5e95f2140789",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal = pd.read_csv('Optimal_feature_set.csv', index_col='Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfb0945-cac7-4634-bbae-50890f815c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_ret_df, opt_size_df = simulation(optimal, 12, 0.05, 0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efca34e8-1671-4636-9251-c586f968dcee",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_ret_df.to_csv('ret_opt.csv', index=True)\n",
    "opt_size_df.to_csv('siz_opt.csv', index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
