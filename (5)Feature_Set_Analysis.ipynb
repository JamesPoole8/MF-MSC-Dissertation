{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa48ac4-6428-4a80-beb0-4a848db4d08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16eace43-f587-4a1f-b2bb-2f2cdbf39084",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import spearmanr\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd6942e-c3b0-4bff-8f4d-123ee7b3ad5f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load Feature Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360230da-4322-424e-8ce7-4dc978f2e230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary of loaded datasets\n",
    "datasets = {\n",
    "    'featuretools_raw': pd.read_csv('cleaned_featuretools_raw.csv', index_col='Date'),\n",
    "    'featuretools_extended': pd.read_csv('cleaned_featuretools_extended.csv', index_col='Date'),\n",
    "    'tsfresh_raw': pd.read_csv('cleaned_tsfresh_raw.csv', index_col='Date'),\n",
    "    'tsfresh_extended': pd.read_csv('cleaned_tsfresh_extended.csv', index_col='Date'),\n",
    "    'featurewiz_raw': pd.read_csv('cleaned_featurewiz_raw.csv', index_col='Date'),\n",
    "    'featurewiz_extended': pd.read_csv('cleaned_featurewiz_extended.csv', index_col='Date'),\n",
    "    'pycaret_raw': pd.read_csv('cleaned_pycaret_raw.csv', index_col='Date'),\n",
    "    'pycaret_extended': pd.read_csv('cleaned_pycaret_extended.csv', index_col='Date'),   \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4c513f-5c31-483f-b0aa-5f43710575aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Names for each dataset for plotting\n",
    "dataset_names = [\n",
    "    \"Featuretools Basic\", \"Featuretools Extended\", \n",
    "    \"TSFresh Basic\", \"TSFresh Extended\",\n",
    "    \"Featurewiz Basic\", \"Featurewiz Extended\",\n",
    "    \"PyCaret Basic\", \"PyCaret Extended\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6c52d3-70ae-4bcc-90ab-b804256109dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_cum_columns(df):\n",
    "    # Drop columns that contain 'CUM' in their names\n",
    "    columns_to_drop = [col for col in df.columns if 'CUM' in col]\n",
    "    df = df.drop(columns=columns_to_drop)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81b1ddf-afc1-495d-9efa-fbe7a628c9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean\n",
    "ft_ext = datasets['featuretools_extended']\n",
    "\n",
    "ft_ext = drop_cum_columns(ft_ext)\n",
    "\n",
    "ft_ext2 = ft_ext.drop(columns=['MODE(metrics_data.ID)', 'MODE(metrics_data.Name)','MONTH(Date)', 'MODE(metrics_data.MONTH(Date))',\n",
    "                              'WEEKDAY(Date)', 'YEAR(Date)', 'MODE(metrics_data.WEEKDAY(Date))', 'MODE(metrics_data.YEAR(Date))'])\n",
    "                     \n",
    "datasets['featuretools_extended'] = ft_ext2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ad1aec-86bf-439e-9a10-2807a7ff8552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean\n",
    "tf_ext = datasets['tsfresh_extended']\n",
    "\n",
    "tf_ext2 = tf_ext.drop(columns=['value__has_duplicate_min', 'value__sum_of_reoccurring_values', \n",
    "                              'value__sum_of_reoccurring_data_points', 'value__value_count__value_1', \n",
    "                              'value__value_count__value_-1','value__has_duplicate', 'value__longest_strike_below_mean', \n",
    "                              'value__count_above_mean', 'value__count_below_mean', 'value__last_location_of_maximum', 'value__first_location_of_maximum', \n",
    "                              'value__symmetry_looking__r_0.05', 'value__large_standard_deviation__r_0.2', 'value__number_cwt_peaks__n_1', 'value__number_peaks__n_5', \n",
    "                              'value__index_mass_quantile__q_0.2', 'value__index_mass_quantile__q_0.3', 'value__index_mass_quantile__q_0.4', 'value__index_mass_quantile__q_0.6', \n",
    "                              'value__index_mass_quantile__q_0.7', 'value__index_mass_quantile__q_0.8','value__index_mass_quantile__q_0.1', 'value__index_mass_quantile__q_0.9',\n",
    "                              'value__last_location_of_minimum', 'value__first_location_of_minimum', 'value__number_cwt_peaks__n_5', 'value__number_peaks__n_1', 'value__number_peaks__n_3', \n",
    "                               'value__change_quantiles__f_agg_\"var\"__isabs_False__qh_0.2__ql_0.0', 'value__change_quantiles__f_agg_\"var\"__isabs_True__qh_0.2__ql_0.0', \n",
    "                               'value__change_quantiles__f_agg_\"var\"__isabs_False__qh_0.4__ql_0.2', 'value__change_quantiles__f_agg_\"var\"__isabs_True__qh_0.4__ql_0.2', \n",
    "                               'value__change_quantiles__f_agg_\"var\"__isabs_False__qh_0.6__ql_0.4', 'value__change_quantiles__f_agg_\"mean\"__isabs_True__qh_0.6__ql_0.4', \n",
    "                               'value__change_quantiles__f_agg_\"var\"__isabs_True__qh_0.6__ql_0.4', 'value__change_quantiles__f_agg_\"var\"__isabs_False__qh_0.8__ql_0.6', \n",
    "                               'value__change_quantiles__f_agg_\"var\"__isabs_True__qh_0.8__ql_0.6', 'value__change_quantiles__f_agg_\"var\"__isabs_False__qh_1.0__ql_0.8', \n",
    "                               'value__change_quantiles__f_agg_\"var\"__isabs_True__qh_1.0__ql_0.8', 'value__value_count__value_0', 'value__number_crossing_m__m_0', \n",
    "                               'value__number_crossing_m__m_-1', 'value__fourier_entropy__bins_3', 'value__fourier_entropy__bins_5', 'value__fourier_entropy__bins_10', 'value__fourier_entropy__bins_100'])\n",
    "                     \n",
    "datasets['tsfresh_extended'] = tf_ext2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5633944a-91d5-42bb-b2bc-58c4cc6984ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d032e2b-09c8-4ce2-aa82-63fcf607db14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to drop low variance columns\n",
    "def drop_low_variance(df, threshold=0.01):\n",
    "    # Select only numeric columns\n",
    "    numeric_df = df.select_dtypes(include=[np.number])\n",
    "    \n",
    "    # Drop low variance columns\n",
    "    selector = VarianceThreshold(threshold=threshold)\n",
    "    selector.fit(numeric_df)\n",
    "    \n",
    "    # Get the mask of retained columns\n",
    "    retained_columns = numeric_df.columns[selector.get_support()]\n",
    "    \n",
    "    # Retain only the columns that passed the variance threshold\n",
    "    df_cleaned = df[retained_columns]\n",
    "    \n",
    "    # Add back any non-numeric columns\n",
    "    non_numeric_df = df.select_dtypes(exclude=[np.number])\n",
    "    df_cleaned = pd.concat([non_numeric_df, df_cleaned], axis=1)\n",
    "    \n",
    "    return df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccba3368-8670-4b49-ba41-3f87c8af83ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to each dataset\n",
    "cleaned_datasets = {name: drop_low_variance(df) for name, df in datasets.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ddf96fc-52c8-4f4a-a5ed-ad3f2ff8866c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Evaluate In-Set Redundancy via PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb384d5c-6bb9-47b3-a440-5d6c5aab732b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform PCA and plot results\n",
    "def pca_plot(datasets, dataset_names):\n",
    "    exp_var_ratio = {}\n",
    "    components = {}\n",
    "\n",
    "    # Plot for cumulative variance explained\n",
    "    plt.figure(figsize=(14, 10))\n",
    "\n",
    "    for i, (name, df) in enumerate(datasets.items()):\n",
    "        # Drop non-numeric columns \n",
    "        df_numeric = df.select_dtypes(include=[np.number])\n",
    "\n",
    "        # Scale the data\n",
    "        scaler = StandardScaler()\n",
    "        scaled_data = scaler.fit_transform(df_numeric)\n",
    "\n",
    "        # Perform PCA\n",
    "        pca = PCA()\n",
    "        pca.fit(scaled_data)\n",
    "\n",
    "        # Calculate cumulative variance explained\n",
    "        explained_variance = np.cumsum(pca.explained_variance_ratio_) * 100\n",
    "        exp_var_ratio[name] = explained_variance\n",
    "\n",
    "        # Plot cumulative variance explained\n",
    "        plt.plot(explained_variance, label=dataset_names[i])\n",
    "\n",
    "        # Find number of components to reach 95% variance\n",
    "        components_95 = np.argmax(explained_variance >= 95) + 1\n",
    "        components[name] = components_95\n",
    "        print(f\"{dataset_names[i]}: {components_95} components to reach 95% variance\")\n",
    "\n",
    "    plt.axhline(y=95, color='red', linestyle='--', linewidth=1)\n",
    "    plt.xlabel('Number of Principal Components')\n",
    "    plt.ylabel('Cumulative Variance Explained (%)')\n",
    "    plt.xlim(0,200)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig('Cumulative_variance.png', dpi=1000)\n",
    "    plt.show()\n",
    "\n",
    "    return exp_var_ratio, components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5782d33-8885-4eee-8464-920b63370c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform PCA and plot results\n",
    "exp_var_ratio, components = pca_plot(cleaned_datasets, dataset_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c366489-10da-4435-9a2c-4f9ad9074aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot for percentage of principal components required for 95% variance\n",
    "plt.figure(figsize=(10, 6))\n",
    "percent_of_components = [components[name] / len(datasets[name].columns) * 100 for name in datasets.keys()]\n",
    "sns.barplot(x=dataset_names, y=percent_of_components)\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel('% of Principal Components for 95% Variance')\n",
    "plt.ylim(0, 55)\n",
    "plt.savefig('Principle_components.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279ae9e0-fe47-4157-8dc2-5a7ce189725b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Evaluate Between-Set Redundancy with Spearman Correlation Coefficient and Similarity Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65eb734c-4154-4248-b72f-7b2334ded985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the datasets to include only the extended ones\n",
    "extended_datasets = {key: value for key, value in cleaned_datasets.items() if 'extended' in key}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74de11b7-45a2-47cb-b6a0-49f8ce32e60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ext_dataset_names = [\"Featuretools Extended\",\"TSFresh Extended\",\"Featurewiz Extended\",\"PyCaret Extended\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b5fdb4-5a9f-488f-a51a-b4ff2da7dc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate similarity between two datasets\n",
    "def calculate_similarity(set1, set2, set1_name, set2_name):\n",
    "    # Initialize correlation matrix\n",
    "    correlations = pd.DataFrame(index=set1.columns, columns=set2.columns)\n",
    "    \n",
    "    # Calculate the Spearman correlation between each pair of features\n",
    "    for col1 in set1.columns:\n",
    "        for col2 in set2.columns:\n",
    "            correlation, _ = spearmanr(set1[col1], set2[col2])\n",
    "            correlations.loc[col1, col2] = correlation\n",
    "    \n",
    "    # Calculate the similarity metric\n",
    "    similarity_metric = correlations.abs().max(axis=1).mean()\n",
    "    \n",
    "    print(f\"Calculated similarity between {set1_name} and {set2_name}: {similarity_metric}\")\n",
    "    \n",
    "    return similarity_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f039f7f-5df3-45ce-ab6a-63e32bf106bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assess across-set redundancy \n",
    "def across_set_red(datasets):\n",
    "    similarity_metrics = {}\n",
    "    dataset_names = list(datasets.keys())\n",
    "    \n",
    "    for i in range(len(dataset_names)):\n",
    "        for j in range(len(dataset_names)):\n",
    "            if i != j:  \n",
    "                set1_name = dataset_names[i]\n",
    "                set2_name = dataset_names[j]\n",
    "                \n",
    "                # Calculate similarity for ordered pair\n",
    "                print(f\"Processing {set1_name} vs {set2_name}\")\n",
    "                similarity_metrics[(set1_name, set2_name)] = calculate_similarity(datasets[set1_name], datasets[set2_name], set1_name, set2_name)\n",
    "                \n",
    "                gc.collect()  \n",
    "    \n",
    "    return similarity_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ffcf9d-bed2-4b4a-9c9c-c6e6e382e3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot heatmap for across-set redundancy \n",
    "def plot_heatmap(similarity_metrics):\n",
    "    # Extract unique feature set names\n",
    "    feature_set_names = sorted(list(set([key[0] for key in similarity_metrics.keys()]).union(set([key[1] for key in similarity_metrics.keys()]))))\n",
    "    \n",
    "    # Initialize the similarity matrix with zeros\n",
    "    similarity_matrix = np.zeros((len(feature_set_names), len(feature_set_names)))\n",
    "\n",
    "    # Fill the similarity matrix with similarity scores\n",
    "    for (set1, set2), similarity in similarity_metrics.items():\n",
    "        i = feature_set_names.index(set1)\n",
    "        j = feature_set_names.index(set2)\n",
    "        similarity_matrix[i, j] = similarity\n",
    "\n",
    "    # Create a mask to hide the diagonal \n",
    "    mask = np.eye(len(feature_set_names), dtype=bool)\n",
    "    \n",
    "    # Plot the heatmap with the mask applied\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(similarity_matrix, xticklabels=feature_set_names, yticklabels=feature_set_names, annot=True, cmap='magma', fmt=\".4f\", mask=mask, vmin=0, vmax=0.1)\n",
    "    plt.title('Across-Set Redundancy Analysis')\n",
    "    plt.xlabel('Feature Set')\n",
    "    plt.ylabel('Feature Set')\n",
    "    plt.savefig('Across_set.png', dpi=1000)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731df54f-eed3-4436-be6d-c7a244da9206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assess across-set redundancy\n",
    "similarity_metrics = across_set_red(extended_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb12793-968e-4157-aa3e-60d4adc04405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot across-set redundancy\n",
    "plot_heatmap(similarity_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45093a3f-611e-462c-b693-6981639f584e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame\n",
    "df = pd.DataFrame.from_dict(similarity_metrics, orient='index', columns=['Similarity'])\n",
    "df.index = pd.MultiIndex.from_tuples(df.index, names=['Set1', 'Set2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0b9b3e-db55-4225-bc4b-bdfd2134b018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV\n",
    "df.to_csv('similarity_metrics.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d729ae8-744f-45cc-a131-0b242c93276d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create Optimal Feature Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d7eae3-7812-4788-af96-0f7e083bbac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8771a6b-0229-46be-aca6-74aca094af0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary of loaded datasets\n",
    "reduced_datasets = {\n",
    "    'raw': pd.read_csv('reduced_raw.csv', index_col='Date'),\n",
    "    'extended': pd.read_csv('reduced_extended.csv', index_col='Date'),\n",
    "    'featuretools_extended': pd.read_csv('reduced_featuretools_extended.csv', index_col='Date'),\n",
    "    'tsfresh_extended': pd.read_csv('reduced_tsfresh_extended.csv', index_col='Date'),\n",
    "    'featurewiz_extended': pd.read_csv('reduced_featurewiz_extended.csv', index_col='Date'),\n",
    "    'pycaret_extended': pd.read_csv('reduced_pycaret_extended.csv', index_col='Date')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2530c2-0544-413f-877f-985ab791ad1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine Extended Feature Sets\n",
    "def combine_features(datasets):\n",
    "    extended_dfs = []\n",
    "    name_id_return_df = None\n",
    "    date_index = None\n",
    "\n",
    "    for name, df in datasets.items():\n",
    "        # Sort by Date and Name to ensure alignment\n",
    "        df = df.sort_values(by=['Date', 'Name'])\n",
    "        \n",
    "        if name_id_return_df is None:\n",
    "            # Keep Name, ID, and Return columns from the first dataset\n",
    "            name_id_return_df = df[['Name', 'ID', 'Return']]\n",
    "            date_index = df.index  \n",
    "\n",
    "        # Drop non-feature columns\n",
    "        df = df.drop(columns=['Name', 'ID', 'Return'], errors='ignore')\n",
    "        extended_dfs.append(df)\n",
    "\n",
    "    # Concatenate all extended feature datasets along the columns\n",
    "    combined_features = pd.concat(extended_dfs, axis=1)\n",
    "    \n",
    "    # Concatenate Name, ID, Return columns back with the combined features\n",
    "    combined_df = pd.concat([name_id_return_df, combined_features], axis=1)\n",
    "    \n",
    "    # Set the Date index back to the combined features dataset\n",
    "    combined_df.index = date_index\n",
    "    \n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e8879d-730e-4aa8-aa5c-cd632ce12042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and combine the extended feature sets\n",
    "combined_df = combine_features(reduced_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ce78c6-248b-4b32-87f6-5f7115c4b36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = combined_df.drop(columns=['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4a609f-ca88-452f-8ecd-51d7df399602",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_corr(df, threshold=0.9):\n",
    "    # Temporarily remove 'Return', 'Name', and 'ID' columns\n",
    "    non_feature_columns = df[['Name', 'ID', 'Return']]\n",
    "    \n",
    "    # Select only numerical columns for correlation analysis\n",
    "    df1 = df.drop(columns=['Return', 'Name', 'ID'])\n",
    "    X = df1.select_dtypes(include=[np.number])\n",
    "    \n",
    "    # Calculate the correlation matrix\n",
    "    corr_matrix = X.corr().abs()\n",
    "    upper_triangle = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    \n",
    "    # Find features with correlation above the threshold\n",
    "    to_drop = [column for column in upper_triangle.columns if any(upper_triangle[column] > threshold)]\n",
    "    \n",
    "    # Drop the highly correlated features\n",
    "    reduced_df = df1.drop(columns=to_drop)\n",
    "    \n",
    "    # Add back the 'Name', 'ID', and 'Return' columns\n",
    "    reduced_df = pd.concat([non_feature_columns, reduced_df], axis=1)\n",
    "    \n",
    "    return reduced_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0539a8b6-d189-48a2-801c-40833dc267a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop highly correlated features\n",
    "df = drop_corr(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a76bae9-35e5-4b12-bde3-a91f617ecac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X = df.drop(columns=['Return', 'Name', 'ID'])  \n",
    "y = df['Return']  \n",
    "\n",
    "# Calculate mutual information scores\n",
    "mi_scores = mutual_info_regression(X, y)\n",
    "\n",
    "# Create a DataFrame to store feature names and their corresponding MI scores\n",
    "mi_df = pd.DataFrame({'Feature': X.columns, 'MI_Score': mi_scores})\n",
    "\n",
    "# Sort features by MI score in descending order\n",
    "mi_df = mi_df.sort_values(by='MI_Score', ascending=False)\n",
    "\n",
    "# Select the top 30 features\n",
    "features = mi_df.head(30)\n",
    "\n",
    "# Get the names of the top 30 features\n",
    "selected_features = features['Feature'].values\n",
    "\n",
    "# Recombine new dataset\n",
    "final_df = df[['Name', 'ID', 'Return'] + list(selected_features)]\n",
    "\n",
    "# Ensure thedataset is indexed by 'Date'\n",
    "final_df.index = df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62af9c6-a2ae-4eb4-840b-b441f4b1570e",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv('Optimal_feature_set.csv', index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
